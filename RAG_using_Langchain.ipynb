{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/RAG-Retrieval-Augmented-Generation-using-Langchain/blob/main/RAG_using_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Installing and importing packages**"
      ],
      "metadata": {
        "id": "0ho6wusQn_ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YmxD_RmDn0ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3aec6f-5bd5-4028-b26b-10f057f1a403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.3-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.3\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langsmith-0.1.63 orjson-3.10.3 packaging-23.2\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.30.3)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (0.1.63)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.46->langchain-openai) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.1.7 tiktoken-0.7.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.11.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.4)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=cd0d161cbd4c2b212014e52d3e52a58611b2ca0b984e6207a37725059da06549\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, ujson, shellingham, python-multipart, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, email_validator, coloredlogs, typer, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, fastapi-cli, opentelemetry-instrumentation-fastapi, fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.18.0 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install langchain-openai langchain_community\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja0NjtqD2Rth",
        "outputId": "6d107d1d-1635-46be-a063-382d983adf0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain_community-0.2.1 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "1KQwtAysn8KC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Authentication for OpenAI API**"
      ],
      "metadata": {
        "id": "5XF7eEn4oaUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive with force remount\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load relevant API Keys\n",
        "file_path = '/content/drive/MyDrive/.API_KEYS/API_KEYS.yml'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    api_keys = yaml.safe_load(file)\n",
        "\n"
      ],
      "metadata": {
        "id": "GdnkoLQvoEKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb84fc3d-a822-4928-e011-8aaf11d14ad8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Extract openai username and key\n",
        "openai_key = api_keys['OPEN_AI']['Key']\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key= os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "# Extract Kaggle username and key\n",
        "aws_access_key_id = api_keys['AWS']['AWS_ACCESS_KEY_ID']\n",
        "aws_secret_access_key = api_keys['AWS']['AWS_SECRET_ACCESS_KEY']\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID']=aws_access_key_id\n",
        "os.environ['AWS_SECRET_ACCESS_KEY']=aws_secret_access_key\n",
        "del aws_access_key_id, aws_secret_access_key\n"
      ],
      "metadata": {
        "id": "fAR1o6r33j7M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install awscli\n",
        "!aws s3 cp s3://datasciencedocx/RAG_using_Langchain/ . --recursive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAGMdU6X478A",
        "outputId": "d8409b91-5af3-4b90-abcb-488891bb65cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: awscli in /usr/local/lib/python3.10/dist-packages (1.32.113)\n",
            "Requirement already satisfied: botocore==1.34.113 in /usr/local/lib/python3.10/dist-packages (from awscli) (1.34.113)\n",
            "Requirement already satisfied: docutils<0.17,>=0.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (0.16)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from awscli) (0.10.1)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.1)\n",
            "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from awscli) (0.4.6)\n",
            "Requirement already satisfied: rsa<4.8,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from awscli) (4.7.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.113->awscli) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.113->awscli) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.113->awscli) (2.0.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.113->awscli) (1.16.0)\n",
            "download: s3://datasciencedocx/RAG_using_Langchain/ens_d2.pdf to ./ens_d2.pdf\n",
            "download: s3://datasciencedocx/RAG_using_Langchain/pca_d1.pdf to ./pca_d1.pdf\n",
            "download: s3://datasciencedocx/RAG_using_Langchain/LangChain.pdf to ./LangChain.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the documents**\n",
        "\n",
        "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)"
      ],
      "metadata": {
        "id": "MifqadjtoW8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF\n",
        "loaders = [\n",
        "    # Duplicate documents on purpose\n",
        "    PyPDFLoader(\"/content/pca_d1.pdf\"),\n",
        "    PyPDFLoader(\"/content/ens_d2.pdf\"),\n",
        "    PyPDFLoader(\"/content/ens_d2.pdf\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "IrWOIYmAofck"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "8N__DNS6r7kK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cfd78a-f74a-4b42-edb9-d43e6b06ea9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "1  \n",
            " N  \n",
            "1 Principal  Component  Analysis  \n",
            "In real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \n",
            "data and find various patterns in it or use it to train some machine learning models.  One way to  \n",
            "think  about  dimensions  is that  suppose  you have  an data  point  x , if we consider  this data  point  as \n",
            "a physical  object  then  dimensions  are merely  a basis  of view,  like where  is the data  located  when  \n",
            "it is observed  from  horizontal  axis or vertical  axis.  \n",
            "As the dimensions  of data  increases,  the difficulty  to visualize  it and perform  computations  on \n",
            "it also increases.  So, how  to reduce  the dimensions  of a data: - \n",
            "• Remove  the redundant  dimensions  \n",
            "• Only keep the most important dimensions  \n",
            "Let us first try to understand  some  terms: - \n",
            "Variance : It is a measure of the variability or it simply measures how spread the data set is.  \n",
            "Mathematically,  it is the average  squared  deviation  from  the mean  score.  We use the following  \n",
            "formula  to compute  variance  var(x).  \n",
            " \n",
            "var(x)  = Σ(xi−x¯)2 \n",
            "N \n",
            "Covariance :  It is a measure of the extent to which corresponding elements from two sets of  \n",
            "ordered data move in the same direction. Formula is shown below denoted by cov(x,y) as the  \n",
            "covariance  of x and y. \n",
            "var(x)  =  Σ(xi−x¯)(yi−y¯) \n",
            "• Here,  xi is the value  of x in ith dimension.  \n",
            "• x bar and y bar denote  the corresponding  mean  values.  \n",
            "• One  way  to observe  the covariance  is how  interrelated  two data  sets are. \n",
            "Positive covariance means X and Y are positively related i.e. as X increases Y also increases.  \n",
            "Negative covariance depicts the exact opposite relation.  However zero covariance means X and Y  \n",
            "are not related.  \n",
            "Now  lets think  about  the requirement  of data  analysis.  \n",
            "Since we try to find the patterns among the data sets so we want the data to be spread out  \n",
            "across each dimension. Also, we want the dimensions to be independent. Such that if data has high  \n",
            "covariance when represented in some n number of dimensions then we replace those dimensions  \n",
            "with linear combination of those n dimensions. Now that data will only be dependent on linear  \n",
            "combination  of those  related  n dimensions.  (related  = have  high  covariance)  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting of document**\n",
        "\n",
        "[Recursively split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "\n",
        "[Split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter)\n"
      ],
      "metadata": {
        "id": "jX09PKcPoZnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "EDOPJOxYtSvt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")"
      ],
      "metadata": {
        "id": "Y1qix4F0saSn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = text_splitter.split_documents(docs)\n",
        "print(len(splits))\n",
        "splits"
      ],
      "metadata": {
        "id": "wHNEkTYTsbKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31083152-c035-46d7-8a81-5292f0317f60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='1  \\n N  \\n1 Principal  Component  Analysis  \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink  about  dimensions  is that  suppose  you have  an data  point  x , if we consider  this data  point  as \\na physical  object  then  dimensions  are merely  a basis  of view,  like where  is the data  located  when', metadata={'source': '/content/pca_d1.pdf', 'page': 0}),\n",
              " Document(page_content='it is observed  from  horizontal  axis or vertical  axis.  \\nAs the dimensions  of data  increases,  the difficulty  to visualize  it and perform  computations  on \\nit also increases.  So, how  to reduce  the dimensions  of a data: - \\n• Remove  the redundant  dimensions  \\n• Only keep the most important dimensions  \\nLet us first try to understand  some  terms: - \\nVariance : It is a measure of the variability or it simply measures how spread the data set is.', metadata={'source': '/content/pca_d1.pdf', 'page': 0}),\n",
              " Document(page_content='Mathematically,  it is the average  squared  deviation  from  the mean  score.  We use the following  \\nformula  to compute  variance  var(x).  \\n \\nvar(x)  = Σ(xi−x¯)2 \\nN \\nCovariance :  It is a measure of the extent to which corresponding elements from two sets of  \\nordered data move in the same direction. Formula is shown below denoted by cov(x,y) as the  \\ncovariance  of x and y. \\nvar(x)  =  Σ(xi−x¯)(yi−y¯) \\n• Here,  xi is the value  of x in ith dimension.', metadata={'source': '/content/pca_d1.pdf', 'page': 0}),\n",
              " Document(page_content='• x bar and y bar denote  the corresponding  mean  values.  \\n• One  way  to observe  the covariance  is how  interrelated  two data  sets are. \\nPositive covariance means X and Y are positively related i.e. as X increases Y also increases.  \\nNegative covariance depicts the exact opposite relation.  However zero covariance means X and Y  \\nare not related.  \\nNow  lets think  about  the requirement  of data  analysis.', metadata={'source': '/content/pca_d1.pdf', 'page': 0}),\n",
              " Document(page_content='Since we try to find the patterns among the data sets so we want the data to be spread out  \\nacross each dimension. Also, we want the dimensions to be independent. Such that if data has high  \\ncovariance when represented in some n number of dimensions then we replace those dimensions  \\nwith linear combination of those n dimensions. Now that data will only be dependent on linear  \\ncombination  of those  related  n dimensions.  (related  = have  high  covariance)', metadata={'source': '/content/pca_d1.pdf', 'page': 0}),\n",
              " Document(page_content='2  \\n  \\nSo, what does  Principal  Component  Analysis  (PCA)  do?  \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread  out data)  \\n \\nHow  does  PCA  work?  \\n• Calculate  the covariance matrix  X of data  points.', metadata={'source': '/content/pca_d1.pdf', 'page': 1}),\n",
              " Document(page_content='• Calculate  eigenvectors  and corresponding  eigenvalues.  \\n• Sort  the eigenvectors  according  to their  eigenvalues  in decreasing  order.  \\n• Choose  first k eigenvectors  and that  will be the new  k dimensions.  \\n• Transform  the original  n dimensional  data  points  into k dimensions.  \\n \\nTo understand the detail working of PCA, we should have knowledge of eigen values and eigen  \\nvectors  \\n \\nEigenvectors:  The directions  in which  our data  are dispersed.', metadata={'source': '/content/pca_d1.pdf', 'page': 1}),\n",
              " Document(page_content='Eigenvalues:  The relative  importance  of these  different  directions.  \\n \\n[Covariance  matrix].[ Eigenvector]  = [eigenvalue].[Eigenvector]  \\nLets  look  into what  a covariance  matrix  is? \\nA covariance  matrix  of some  data  set in 4 dimensions  a,b,c,d.  \\n□ Va Ca,b Ca,c Ca,d Ca,e \\n \\n□ Ca,b Vb Cb,c Cb,d Cb,e \\n \\n \\n \\nVa : variance  along  dimension  a \\nCa,b  : Covariance  along  dimension  a and b', metadata={'source': '/content/pca_d1.pdf', 'page': 1}),\n",
              " Document(page_content='Ca,b  : Covariance  along  dimension  a and b \\nIf we have  a matrix  X of m*n  dimension  such  that  it holds  n data  points  of m dimensions,  then  \\ncovariance  matrix  can be calculated  as \\nC   =    1  (X − X¯ )(X − X¯ )T \\n x n−1 \\nIt is important  to note  that  the covariance  matrix  contains: - \\n• variance of  dimensions  as the main  diagonal  elements.  \\n• covariance  of dimensions  as the off diagonal  elements.', metadata={'source': '/content/pca_d1.pdf', 'page': 1}),\n",
              " Document(page_content='Also,  covariance  matrix  is symmetric  (observe  from  the image  above)  Ca,c Cb,c Vc Cc,d Cc e \\nCa,d Cb,d Cc,d Vd Cd e \\nCa,e Cb,e Cc,e Cd,e Ve', metadata={'source': '/content/pca_d1.pdf', 'page': 1}),\n",
              " Document(page_content='3  \\n  \\nAs, we discussed earlier we want the data to be spread out i.e. it should have high variance along  \\ndimensions. Also  we want to remove correlated dimensions i.e. covariance among the dimensions  \\nshould  be zero  (they  should  be linearly  independent).  \\nTherefore,  our covariance  matrix  should  have: - \\n• large  numbers  as the main  diagonal  elements.  \\n• zero  values  as the off diagonal  elements.', metadata={'source': '/content/pca_d1.pdf', 'page': 2}),\n",
              " Document(page_content='• zero  values  as the off diagonal  elements.  \\nWe call it a diagonal matrix. So, we have to transform the original data points such that their  \\ncovariance  is a diagonal  matrix.  \\nAlways  normalize  your  data  before  doing  PCA if we use data  (features  here)  of different  scales,  we \\nget misleading components. We can also simply use correlation matrix instead of using covariance  \\nmatrix  if features  are of different  scales.  \\nThis defines  the goal  of PCA: -', metadata={'source': '/content/pca_d1.pdf', 'page': 2}),\n",
              " Document(page_content='This defines  the goal  of PCA: - \\n1. Find  linearly  independent  dimensions  which  can losslessly  represent  the data  points.  \\n2. Those  newly  found  dimensions  should  allow  us to predict/reconstruct  the original  dimensions.', metadata={'source': '/content/pca_d1.pdf', 'page': 2}),\n",
              " Document(page_content='1   \\nEnsemble  Methods  \\nLet us consider  a real  world  situation  which  uses  Ensemble  Methods,  which  is, when  a user  wants  \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive  or negative  ratings.  If in the group,  many  users  have  given  positive  ratings,  then  the \\ncombined  rating  will be positive.  Instead  of a single  rating,  the ratings  of the group  of users  is', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='considered.  The product  is bought  by the user  when  the combined  ratings  of the group  is positive.  \\nThe user  gets  a fairer  idea  about  the product  when  all the ratings  are combined.  \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy.  \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore  powerful  prediction.', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='more  powerful  prediction.  \\nThus,  ensemble  methods  increase  the accuracy  of the predictions.  \\n \\nWhy  use Ensemble  Methods?  \\nEnsemble  Methods  are used  in order  to: \\n• decrease  variance  (bagging)  \\n• decrease  bias (boosting)  \\n• improve  predictions  (stacking)  \\n \\nBagging  \\nBagging  actually  refers  to Bootstrap  Aggregators.  \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap -', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='ping.  In turn,  this reduces  the noise  and variance  by utilizing  multiple  samples.  Each  hypothesis  \\nhas the same  weight  as all the others.  Now,  aggregating  of the outputs  of various  models  is done.  \\n \\nBoosting  \\nBoosting is an iterative technique which adjusts the weight of an observation based on the last  \\nclassification. If an observation was classified incorrectly, it tries to increase the weight of this', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='observation and vice versa. Boosting in general decreases the bias error and builds strong predictive  \\nmodels.  \\n \\nVariance  \\nVariance quantifies how the predictions made on same observation are different from each other. A  \\nhigh variance model will over -fit on your training population and perform badly on any observation  \\nbeyond  training.  Thus,  we aim at low variance.', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='2   \\nBias  \\nBias error is useful to quantify how much on on average are the predicted values different from the  \\nactual value. A high  bias  error means we have a under -performing model.  Thus,  we aim at low  \\nbias.  \\nA commonly  used  class  of ensemble  methods  are forests  of randomized  trees.  \\nIn random  forests,  each  tree  in the ensemble  is built  from  a sample  drawn  with  replacement  (i.e.', metadata={'source': '/content/ens_d2.pdf', 'page': 1}),\n",
              " Document(page_content='a bootstrap sample) from the training set. In addition, instead of using all the features, a random  \\nsubset  of features  is selected,  further  randomizing  the tree.  \\nAs a result, the bias of the forest increases slightly, but due to the averaging of less correlated  \\ntrees,  its variance  decreases,  resulting  in an overall  better  model.', metadata={'source': '/content/ens_d2.pdf', 'page': 1}),\n",
              " Document(page_content='1   \\nEnsemble  Methods  \\nLet us consider  a real  world  situation  which  uses  Ensemble  Methods,  which  is, when  a user  wants  \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive  or negative  ratings.  If in the group,  many  users  have  given  positive  ratings,  then  the \\ncombined  rating  will be positive.  Instead  of a single  rating,  the ratings  of the group  of users  is', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='considered.  The product  is bought  by the user  when  the combined  ratings  of the group  is positive.  \\nThe user  gets  a fairer  idea  about  the product  when  all the ratings  are combined.  \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy.  \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore  powerful  prediction.', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='more  powerful  prediction.  \\nThus,  ensemble  methods  increase  the accuracy  of the predictions.  \\n \\nWhy  use Ensemble  Methods?  \\nEnsemble  Methods  are used  in order  to: \\n• decrease  variance  (bagging)  \\n• decrease  bias (boosting)  \\n• improve  predictions  (stacking)  \\n \\nBagging  \\nBagging  actually  refers  to Bootstrap  Aggregators.  \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap -', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='ping.  In turn,  this reduces  the noise  and variance  by utilizing  multiple  samples.  Each  hypothesis  \\nhas the same  weight  as all the others.  Now,  aggregating  of the outputs  of various  models  is done.  \\n \\nBoosting  \\nBoosting is an iterative technique which adjusts the weight of an observation based on the last  \\nclassification. If an observation was classified incorrectly, it tries to increase the weight of this', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='observation and vice versa. Boosting in general decreases the bias error and builds strong predictive  \\nmodels.  \\n \\nVariance  \\nVariance quantifies how the predictions made on same observation are different from each other. A  \\nhigh variance model will over -fit on your training population and perform badly on any observation  \\nbeyond  training.  Thus,  we aim at low variance.', metadata={'source': '/content/ens_d2.pdf', 'page': 0}),\n",
              " Document(page_content='2   \\nBias  \\nBias error is useful to quantify how much on on average are the predicted values different from the  \\nactual value. A high  bias  error means we have a under -performing model.  Thus,  we aim at low  \\nbias.  \\nA commonly  used  class  of ensemble  methods  are forests  of randomized  trees.  \\nIn random  forests,  each  tree  in the ensemble  is built  from  a sample  drawn  with  replacement  (i.e.', metadata={'source': '/content/ens_d2.pdf', 'page': 1}),\n",
              " Document(page_content='a bootstrap sample) from the training set. In addition, instead of using all the features, a random  \\nsubset  of features  is selected,  further  randomizing  the tree.  \\nAs a result, the bias of the forest increases slightly, but due to the averaging of less correlated  \\ntrees,  its variance  decreases,  resulting  in an overall  better  model.', metadata={'source': '/content/ens_d2.pdf', 'page': 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embeddings**\n",
        "\n",
        "Let's take our splits and embed them."
      ],
      "metadata": {
        "id": "HRYXCWXxsmH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "UAy8WJblsiXH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paxP3GEUuSaS",
        "outputId": "768eb060-6c39-4ebf-9124-ffa80eac7b77"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7904a0341630>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7904a0377520>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding similarity search with a toy example**"
      ],
      "metadata": {
        "id": "OtqIaNDTsq3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"i like dogs\"\n",
        "sentence2 = \"i like cats\"\n",
        "sentence3 = \"the weather is ugly, too hot outside\""
      ],
      "metadata": {
        "id": "prWJAL50szYO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding1 = embedding.embed_query(sentence1)\n",
        "embedding2 = embedding.embed_query(sentence2)\n",
        "embedding3 = embedding.embed_query(sentence3)"
      ],
      "metadata": {
        "id": "iAeeFUsis1wO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding1), len(embedding2), len(embedding3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGc4yWSas4RX",
        "outputId": "0568a0fc-d43b-4064-db4f-581f5b6c4397"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1536, 1536, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check 3///;[[]] similarity45\n",
        "np.dot(embedding1, embedding2), np.dot(embedding1, embedding3),np.dot(embedding2, embedding3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDbU70Zss6j4",
        "outputId": "1e661c74-34a8-4ea1-c42b-719bb3158500"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9200098139975112, 0.7636469209069651, 0.758339267629513)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vectorstores**"
      ],
      "metadata": {
        "id": "osU6iYMLswJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma # Light-weight and in memory"
      ],
      "metadata": {
        "id": "lIFZ53aAtLEw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'docs/chroma/'\n",
        "!rm -rf ./docs/chroma  # remove old database files if any"
      ],
      "metadata": {
        "id": "oI9whQjptqav"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits, # splits we created earlier\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory # save the directory\n",
        ")"
      ],
      "metadata": {
        "id": "BPeW91Qdtxmw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist() # Let's **save vectordb** so we can use it later!"
      ],
      "metadata": {
        "id": "HncL1qsEt3RQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9657092-c646-4494-83ec-743dec572ed7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectordb._collection.count()) # same as number of splites"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxEeyHgjt0Po",
        "outputId": "467368a9-50b5-40a6-8535-6aa9739f7c5c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Similarity Search**"
      ],
      "metadata": {
        "id": "0-2WgevOt_TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how does pca reduce the dimension?\""
      ],
      "metadata": {
        "id": "_OUYY3Mot2nL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectordb.similarity_search(question,k=3) # k --> No. of doc as return\n",
        "print(len(docs))\n",
        "print(docs[0].page_content)\n",
        "print(docs[1].page_content)\n",
        "print(docs[2].page_content)"
      ],
      "metadata": {
        "id": "DGHDzVj4uyx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe52779-b047-4df0-b751-f0966f97d5d8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "This defines  the goal  of PCA: - \n",
            "1. Find  linearly  independent  dimensions  which  can losslessly  represent  the data  points.  \n",
            "2. Those  newly  found  dimensions  should  allow  us to predict/reconstruct  the original  dimensions.\n",
            "2  \n",
            "  \n",
            "So, what does  Principal  Component  Analysis  (PCA)  do?  \n",
            "PCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \n",
            "orthogonal (and hence linearly independent) and ranked according to the variance of data along  \n",
            "them. It means more important principle axis occurs first. (more important = more variance/more  \n",
            "spread  out data)  \n",
            " \n",
            "How  does  PCA  work?  \n",
            "• Calculate  the covariance matrix  X of data  points.\n",
            "1  \n",
            " N  \n",
            "1 Principal  Component  Analysis  \n",
            "In real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \n",
            "data and find various patterns in it or use it to train some machine learning models.  One way to  \n",
            "think  about  dimensions  is that  suppose  you have  an data  point  x , if we consider  this data  point  as \n",
            "a physical  object  then  dimensions  are merely  a basis  of view,  like where  is the data  located  when\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Edge case where failure may happen**\n",
        "\n",
        "1. Lack of Diversity : Semantic search fetches all similar documents, but does not enforce diversity.\n",
        "\n",
        "    - Notice that we're getting duplicate chunks (because of the duplicate `ens_d2.pdf` in the index). `docs[0]` and `docs[1]` are indentical.\n",
        "\n",
        "  **Addressing Diversity - MMR-Maximum Marginal Relevance**\n",
        "\n",
        "2. Lack of spefificity:  The question may be from a particular doc but answer may contain information from other doc.\n",
        "\n",
        "  **Addressing Specificity: Working with metadata - Manually**\n",
        "\n",
        "  **Working with metadata using self-query retriever -Automatically**"
      ],
      "metadata": {
        "id": "qBh5fXjrwHw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **Example 1. Addressing Diversity - MMR-Maximum Marginal Relevance**"
      ],
      "metadata": {
        "id": "UJNlYo983oZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question= 'how ensemble method works?'\n",
        "docs = vectordb.similarity_search(question,k=5) # Without MMR"
      ],
      "metadata": {
        "id": "TBdrnHC3uy0n"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9huk6xy3xWW",
        "outputId": "d48cff9c-47e9-4bc5-a8fb-73010a4e3cbd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1   \\nEnsemble  Methods  \\nLet us consider  a real  world  situation  which  uses  Ensemble  Methods,  which  is, when  a user  wants  \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive  or negative  ratings.  If in the group,  many  users  have  given  positive  ratings,  then  the \\ncombined  rating  will be positive.  Instead  of a single  rating,  the ratings  of the group  of users  is', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHBRG8p-4RF6",
        "outputId": "548a36fa-3f62-442f-fb86-b55a3fa7e224"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1   \\nEnsemble  Methods  \\nLet us consider  a real  world  situation  which  uses  Ensemble  Methods,  which  is, when  a user  wants  \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive  or negative  ratings.  If in the group,  many  users  have  given  positive  ratings,  then  the \\ncombined  rating  will be positive.  Instead  of a single  rating,  the ratings  of the group  of users  is', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPbBVNYW65d-",
        "outputId": "dc126626-ea37-49e5-9f0f-d521c0ca04b7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='considered.  The product  is bought  by the user  when  the combined  ratings  of the group  is positive.  \\nThe user  gets  a fairer  idea  about  the product  when  all the ratings  are combined.  \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy.  \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore  powerful  prediction.', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmCmTCnR65UE",
        "outputId": "95333dea-f2b7-4267-9842-cf46cd2c85d1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='considered.  The product  is bought  by the user  when  the combined  ratings  of the group  is positive.  \\nThe user  gets  a fairer  idea  about  the product  when  all the ratings  are combined.  \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy.  \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore  powerful  prediction.', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addressing Diversity - MMR-Maximum Marginal Relevance"
      ],
      "metadata": {
        "id": "ns8VCrc25Y2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr=vectordb.max_marginal_relevance_search(question, k=3, fetch_k=6) # With MMR"
      ],
      "metadata": {
        "id": "Qg70_vhh5e7x"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r_KetcH5wDj",
        "outputId": "2bc661f9-20e1-4bbb-d857-0aec06d50aad"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1   \\nEnsemble  Methods  \\nLet us consider  a real  world  situation  which  uses  Ensemble  Methods,  which  is, when  a user  wants  \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive  or negative  ratings.  If in the group,  many  users  have  given  positive  ratings,  then  the \\ncombined  rating  will be positive.  Instead  of a single  rating,  the ratings  of the group  of users  is', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vppdq9Bn5y0z",
        "outputId": "6b341122-1eef-4d43-d564-d09cc8677dc4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='considered.  The product  is bought  by the user  when  the combined  ratings  of the group  is positive.  \\nThe user  gets  a fairer  idea  about  the product  when  all the ratings  are combined.  \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy.  \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore  powerful  prediction.', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLqM_I87-thH",
        "outputId": "d3138e54-945e-40e0-8605-5f52404d0634"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='more  powerful  prediction.  \\nThus,  ensemble  methods  increase  the accuracy  of the predictions.  \\n \\nWhy  use Ensemble  Methods?  \\nEnsemble  Methods  are used  in order  to: \\n• decrease  variance  (bagging)  \\n• decrease  bias (boosting)  \\n• improve  predictions  (stacking)  \\n \\nBagging  \\nBagging  actually  refers  to Bootstrap  Aggregators.  \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap -', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Example 2. Addressing Specificity: Working with metadata - Manually**"
      ],
      "metadata": {
        "id": "vZYR1s6x_bpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without metadata information\n",
        "question = \"what is the role of variance in pca?\"\n",
        "docs = vectordb.similarity_search(question,k=5)\n",
        "for doc in docs:\n",
        "    print(doc.metadata) # metadata contains information about from which doc the answer has been fetched"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GzDPqSVAX6Y",
        "outputId": "282a3006-6753-4aff-85c1-bb7ff0c5f2b9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 1, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 2, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 2, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 0, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 0, 'source': '/content/ens_d2.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice above, the last information is from 'ens_d2' doc."
      ],
      "metadata": {
        "id": "sMv4a6P_A0rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With metadata information\n",
        "question = \"what is the role of variance in pca?\"\n",
        "docs = vectordb.similarity_search(\n",
        "    question,\n",
        "    k=5,\n",
        "    filter={\"source\":'/content/pca_d1.pdf'} # manually passing metadata, using metadata filter.\n",
        ")\n",
        "\n",
        "for doc in docs:\n",
        "    print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-9NluQR_VUi",
        "outputId": "6f187b9f-d6bd-4300-b8bd-0483f127f441"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 1, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 2, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 2, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 0, 'source': '/content/pca_d1.pdf'}\n",
            "{'page': 0, 'source': '/content/pca_d1.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Addressing Specificity -Automatically: Working with metadata using self-query retriever**](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query)\n",
        "\n"
      ],
      "metadata": {
        "id": "tiFz42WiCJUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Additional tricks: Compression**\n",
        "\n",
        "Another approach for improving the quality of retrieved docs is compression. Information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "[Contextual compression](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression) is meant to fix this."
      ],
      "metadata": {
        "id": "qQG1e-boGTTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieval + Question Answering :  Connecting with LLMs**"
      ],
      "metadata": {
        "id": "2aIGynCLGw0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehEr-pUMGMrK",
        "outputId": "ce6eeb94-65d8-456b-d837-04235c897686"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is principal component analysis?\"\n",
        "docs = vectordb.max_marginal_relevance_search(question, k=2, fetch_k=5)\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc8EF156G3uv",
        "outputId": "27fa2c5f-83c6-4d36-be05-ec4a0a5eb56d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpV03kILH6vl",
        "outputId": "2bd80a70-3935-467a-f06c-e9038220f794"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='2  \\n  \\nSo, what does  Principal  Component  Analysis  (PCA)  do?  \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread  out data)  \\n \\nHow  does  PCA  work?  \\n• Calculate  the covariance matrix  X of data  points.', metadata={'page': 1, 'source': '/content/pca_d1.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr4vt1ArIEWX",
        "outputId": "81574834-db54-48f2-a134-09a0257fe870"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1  \\n N  \\n1 Principal  Component  Analysis  \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink  about  dimensions  is that  suppose  you have  an data  point  x , if we consider  this data  point  as \\na physical  object  then  dimensions  are merely  a basis  of view,  like where  is the data  located  when', metadata={'page': 0, 'source': '/content/pca_d1.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**[RetrievalQA chain](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain)**\n",
        "\n",
        "####**[Vector store-backed retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)**"
      ],
      "metadata": {
        "id": "eC-7gtzfHQYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
      ],
      "metadata": {
        "id": "vVMm3gMzG-4v"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "PnUVz2UmHWAJ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is principal component analysis?\"\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True)\n",
        "\n",
        "result = qa_chain.invoke({\"query\": question})"
      ],
      "metadata": {
        "id": "oFygf_Y7HWQ5"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "A-yxS2awHWT7",
        "outputId": "b5d9149d-142c-4594-ed76-5cc2779a61d8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a technique used in data analysis to find a new set of dimensions that are orthogonal (linearly independent) and ranked according to the variance of data along them. The goal of PCA is to find linearly independent dimensions that can represent the data points losslessly and allow for the prediction or reconstruction of the original dimensions. PCA works by calculating the covariance matrix of data points, finding eigenvectors and corresponding eigenvalues, sorting the eigenvectors based on eigenvalues, choosing the top k eigenvectors as new dimensions, and transforming the original n-dimensional data points into k dimensions. In simpler terms, PCA helps in reducing the dimensionality of data while preserving the most important information or patterns in the data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBOgOkjPOvmO",
        "outputId": "6e03fd7a-eae9-4aaa-f051-5543d83ae927"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='2  \\n  \\nSo, what does  Principal  Component  Analysis  (PCA)  do?  \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread  out data)  \\n \\nHow  does  PCA  work?  \\n• Calculate  the covariance matrix  X of data  points.', metadata={'page': 1, 'source': '/content/pca_d1.pdf'}),\n",
              " Document(page_content='1  \\n N  \\n1 Principal  Component  Analysis  \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink  about  dimensions  is that  suppose  you have  an data  point  x , if we consider  this data  point  as \\na physical  object  then  dimensions  are merely  a basis  of view,  like where  is the data  located  when', metadata={'page': 0, 'source': '/content/pca_d1.pdf'}),\n",
              " Document(page_content='This defines  the goal  of PCA: - \\n1. Find  linearly  independent  dimensions  which  can losslessly  represent  the data  points.  \\n2. Those  newly  found  dimensions  should  allow  us to predict/reconstruct  the original  dimensions.', metadata={'page': 2, 'source': '/content/pca_d1.pdf'}),\n",
              " Document(page_content='• Calculate  eigenvectors  and corresponding  eigenvalues.  \\n• Sort  the eigenvectors  according  to their  eigenvalues  in decreasing  order.  \\n• Choose  first k eigenvectors  and that  will be the new  k dimensions.  \\n• Transform  the original  n dimensional  data  points  into k dimensions.  \\n \\nTo understand the detail working of PCA, we should have knowledge of eigen values and eigen  \\nvectors  \\n \\nEigenvectors:  The directions  in which  our data  are dispersed.', metadata={'page': 1, 'source': '/content/pca_d1.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Under the hood? --> Understanding RAG Prompt**"
      ],
      "metadata": {
        "id": "Y-ea4zDtRnYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchainhub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TLWjL0jRzeq",
        "outputId": "4820a2e5-0eb4-472a-f311-580800c4abf9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.16-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.16 types-requests-2.32.0.20240523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2USwNz7lRn0y",
        "outputId": "d4bb6302-9069-434c-b47e-73a175c66c91"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use three sentences maximum.Keep the answer as concise as possible."
      ],
      "metadata": {
        "id": "BqGrJNIWLqks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
      ],
      "metadata": {
        "id": "tB6IsTdSLvgs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA_CHAIN_PROMPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQasHvgTSX92",
        "outputId": "8486e4ae-30de-42f0-a03c-f98016f585bd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], template='Use the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nAlways say \"thanks for asking!\" at the end of the answer.\\n{context}\\nQuestion: {question}\\nHelpful Answer:')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 2, \"fetch_k\":6} ), # \"k\":2, \"fetch_k\":3\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        "                                       return_source_documents=True\n",
        "                                       )"
      ],
      "metadata": {
        "id": "-LJ3BqKjL89C"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTMm7LokcWdB",
        "outputId": "0881ac17-214e-44ea-c151-0dacd2e9a468"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Use the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nAlways say \"thanks for asking!\" at the end of the answer.\\n{context}\\nQuestion: {question}\\nHelpful Answer:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79049c6fc970>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79049c6fece0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7904a04e9060>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 6}))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1**"
      ],
      "metadata": {
        "id": "SHn6k5ePZ2oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is principal component analysis?\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGORZtkrMvx3",
        "outputId": "8698247f-de0a-4c7f-b722-ba08089491f2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='2  \\n  \\nSo, what does  Principal  Component  Analysis  (PCA)  do?  \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread  out data)  \\n \\nHow  does  PCA  work?  \\n• Calculate  the covariance matrix  X of data  points.', metadata={'page': 1, 'source': '/content/pca_d1.pdf'}),\n",
              " Document(page_content='1  \\n N  \\n1 Principal  Component  Analysis  \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink  about  dimensions  is that  suppose  you have  an data  point  x , if we consider  this data  point  as \\na physical  object  then  dimensions  are merely  a basis  of view,  like where  is the data  located  when', metadata={'page': 0, 'source': '/content/pca_d1.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-FxSBsmdXtgz",
        "outputId": "88075f85-e9b7-4008-bf41-b61c9f2de23e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a technique used to find a new set of dimensions that are orthogonal and ranked according to the variance of data along them. The more important principle axis occurs first, based on the variance or spread out data. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2**"
      ],
      "metadata": {
        "id": "MV-VTth3Zq6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What does it say about variance in context of both PCA and Ensemble?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "J1LmenL3ZNrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9537544b-d6ba-429e-d4be-ad1c9fead2ce"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='2  \\n  \\nSo, what does  Principal  Component  Analysis  (PCA)  do?  \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread  out data)  \\n \\nHow  does  PCA  work?  \\n• Calculate  the covariance matrix  X of data  points.', metadata={'page': 1, 'source': '/content/pca_d1.pdf'}),\n",
              " Document(page_content='more  powerful  prediction.  \\nThus,  ensemble  methods  increase  the accuracy  of the predictions.  \\n \\nWhy  use Ensemble  Methods?  \\nEnsemble  Methods  are used  in order  to: \\n• decrease  variance  (bagging)  \\n• decrease  bias (boosting)  \\n• improve  predictions  (stacking)  \\n \\nBagging  \\nBagging  actually  refers  to Bootstrap  Aggregators.  \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap -', metadata={'page': 0, 'source': '/content/ens_d2.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "i1w5C9OOZjEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "852d4334-ba42-4240-aee1-3a589a4e59cf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Both PCA and Ensemble methods, such as Bagging, aim to decrease variance. PCA does this by finding a new set of dimensions that are orthogonal and ranked according to the variance of the data, while Ensemble methods like Bagging decrease variance by testing multiple models on the data through sampling and replacing data. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RetrievalQA chain types : [Map reduce, Refine, Map rerank(Legacy)](https://python.langchain.com/docs/modules/chains/)**\n",
        "\n",
        "- Whatever techniques we havae used is stuff method (default - chain_type=\"stuff\")and there is only one call to LLM"
      ],
      "metadata": {
        "id": "ksvPzTKHdjYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 4, \"fetch_k\":8}),\n",
        "    chain_type=\"map_reduce\"\n",
        ")"
      ],
      "metadata": {
        "id": "_gJGOi7vczPO"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"What principal component analysis?\"\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "ELbLfWWAdxOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "d6b5edb9-5e7a-452b-e75d-cc5de9cdc8a2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a statistical technique used to simplify and reduce the dimensionality of data by finding a new set of dimensions that are orthogonal (linearly independent) and ranked according to the variance of the data along them. PCA helps in identifying the most important axes or components that capture the most variation in the data. It is commonly used in data analysis to reduce complexity, visualize data, identify patterns, and extract important features in various fields such as machine learning, image processing, and finance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Make it like Chatbot : Adding Memory**"
      ],
      "metadata": {
        "id": "7NbLm1boaRT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "Ngcffei9fHNm"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "qa= ConversationalRetrievalChain.from_llm(llm,\n",
        "                                       retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 4, \"fetch_k\":8} ), # \"k\":2, \"fetch_k\":3\n",
        "                                       memory=memory\n",
        "                                       )"
      ],
      "metadata": {
        "id": "L994Ysxkff9S"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"tell me something about PCA\"\n",
        "result = qa.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "3jkbHJvXf9Ai"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "BHxC5yLRhLHL",
        "outputId": "97e0bec3-2d00-42f5-c58f-0d909900f36c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a technique used in data analysis and machine learning to simplify the complexity of high-dimensional data by transforming it into a new set of dimensions called principal components. These components are orthogonal (linearly independent) and are ranked based on the variance of the data along them. The more important principal axes, which capture the most variance or spread out the data, occur first.\\n\\nPCA works by first calculating the covariance matrix of the data points. The main goals of PCA are to find linearly independent dimensions that can represent the data losslessly and to allow the prediction or reconstruction of the original dimensions. This is achieved by calculating eigenvectors and corresponding eigenvalues, sorting the eigenvectors based on their eigenvalues in decreasing order, choosing the first k eigenvectors as the new k dimensions, and transforming the original n-dimensional data points into k dimensions.\\n\\nEigenvectors represent the directions in which the data are dispersed. It is important to normalize the data before applying PCA, especially if the features have different scales, to avoid misleading components. Additionally, using the correlation matrix instead of the covariance matrix can be beneficial when dealing with features of different scales.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"please list point-wise,  how does pca works?\"\n",
        "result = qa({\"question\": question})"
      ],
      "metadata": {
        "id": "dN3atqo6hfc4"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z7pSOdThii4",
        "outputId": "4d2bf489-fee9-49dc-8024-bfb439a365dd"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component Analysis (PCA) works in the following steps:\n",
            "\n",
            "1. Calculate the covariance matrix X of data points.\n",
            "2. Find linearly independent dimensions that can losslessly represent the data points.\n",
            "3. Calculate eigenvectors and corresponding eigenvalues.\n",
            "4. Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
            "5. Choose the first k eigenvectors, which will be the new k dimensions.\n",
            "6. Transform the original n-dimensional data points into k dimensions.\n",
            "\n",
            "It is important to understand eigenvalues and eigenvectors in order to grasp the detailed working of PCA. Eigenvectors represent the directions in which our data are dispersed. By transforming the original data points, the goal is to achieve a diagonal covariance matrix. It is recommended to normalize the data before performing PCA to avoid misleading components, especially when using features of different scales. Additionally, using a correlation matrix instead of a covariance matrix can be beneficial when dealing with features of varying scales.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what do we get from covariance matrix for doing PCA?\"\n",
        "result = qa({\"question\": question})\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "id": "rH_Z97sJiWLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3da3d2-069e-45be-fcca-b4d733b05f2f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When performing Principal Component Analysis (PCA), the covariance matrix provides information about the relationships between the different dimensions or features in the dataset. Specifically, the covariance matrix helps in identifying how the dimensions are related to each other in terms of variance and covariance. By analyzing the covariance matrix, PCA determines the directions in which the data varies the most, which are represented by the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components (new set of dimensions) that are orthogonal and ranked based on the variance of the data along them, while the eigenvalues indicate the relative importance of these different directions. In summary, the covariance matrix in PCA helps in understanding the structure and variability of the data in order to find the most important dimensions for dimensionality reduction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download the vector DB**"
      ],
      "metadata": {
        "id": "GHJ1ujNcuFNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the entire folder\n",
        "!zip -r /content/docs.zip /content/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Defg2TBIuIeB",
        "outputId": "fe3b82de-e805-40e3-bec0-c0f8ad6bb29d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/docs/ (stored 0%)\n",
            "  adding: content/docs/chroma/ (stored 0%)\n",
            "  adding: content/docs/chroma/chroma.sqlite3 (deflated 58%)\n",
            "  adding: content/docs/chroma/e6a68376-3554-4f31-99fa-7204090f366d/ (stored 0%)\n",
            "  adding: content/docs/chroma/e6a68376-3554-4f31-99fa-7204090f366d/link_lists.bin (stored 0%)\n",
            "  adding: content/docs/chroma/e6a68376-3554-4f31-99fa-7204090f366d/data_level0.bin (deflated 100%)\n",
            "  adding: content/docs/chroma/e6a68376-3554-4f31-99fa-7204090f366d/header.bin (deflated 61%)\n",
            "  adding: content/docs/chroma/e6a68376-3554-4f31-99fa-7204090f366d/length.bin (deflated 98%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/docs.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wm3nYLKJuKYq",
        "outputId": "9a14e980-ac0d-4a5e-f8c2-8283b0297680"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d581715-fd1a-4178-aea0-9bd9ed994514\", \"docs.zip\", 204438)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Upload the vector db from previous step and unzip**"
      ],
      "metadata": {
        "id": "BMa6GdcUuXcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/docs.zip  -d /"
      ],
      "metadata": {
        "id": "VXn0d0k6ubPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5d8153-a049-46d8-bcd4-10d8af982a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/docs.zip\n",
            "replace /content/docs/chroma/chroma.sqlite3? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Ps-q_cK_1Vt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}